{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be building a predictive model for survival on the titanic based on training data provided by kaggle. This is part of the Warmup Project for Data Science 2016. \n",
    "\n",
    "#### A. import the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pandas.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# Uncomment print statements below to take a look at the \n",
    "# first 5 rows of the dataframe and the describing output.\n",
    "# print(titanic.head(5))\n",
    "# print(titanic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. clean up the missing data. \n",
    "\n",
    "Occasionally a dataset contains missing values (null, not a number, NA, etc.) and we want to prevent these missing values from affecting our computations in unintended ways. In particular, this training data set has missing values for `Age`, so let's clean that up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. convert non-numeric (categorical) variables into usable numbers!\n",
    "\n",
    "In particular, `Sex` and `Embarked` should be converted into usable numbers. We'll find all the unique values for these non-numeric data points and replace them with numbers that can be used by the predictive model in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique genders are ['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique genders \n",
    "print\"unique genders are\", titanic[\"Sex\"].unique()\n",
    "\n",
    "# From genders to numbers\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique embarked values are ['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Find all the uniqued embarked values\n",
    "print \"unique embarked values are\", titanic[\"Embarked\"].unique()\n",
    "\n",
    "# From embarked letters to numbers\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. cross validation, linear regression, first stab at predictions \n",
    "\n",
    "We want to make sure that we don't train our model on the same data that we'll make predictions on, so we're going to split the data into several folds. In each trial, one fold will be set aside for predictions, and the remaining folds will be used for training. Thus there's no overlap between the folds/partitions that were used for training and the one fold used for predictions. We'll run several trials with these fold combinations and eventually get predictions for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code from dataquest mission 74, part 9.\n",
    "\n",
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.08998778098664095, 0.9607562062264069, 0.5926762776633059,\n",
      "       0.9311387278910165, 0.05293430709559788, 0.17027568492773304,\n",
      "       0.36994359030196483, 0.10347484742579771, 0.5215979058146003,\n",
      "       0.8744910503206457, 0.6488836111621734, 0.8297427688347156,\n",
      "       0.1347971983986338, -0.1611268436425003, 0.6581413066296763,\n",
      "       0.6398197484682686, 0.15173387493789559, 0.29543271790803804,\n",
      "       0.5353779589276406, 0.6210076833082576, 0.2618725916383594,\n",
      "       0.2626875613868237, 0.7317391597365706, 0.5059958971692413,\n",
      "       0.5613985666552621, 0.33503973416336075, 0.13033880755757876,\n",
      "       0.4687657665282141, 0.660737752649522, 0.09108192184311348,\n",
      "       0.47722392003543795, 1.0422002619036157, 0.6606916127819125,\n",
      "       0.08715392731116645, 0.5285507322778327, 0.40187433784208143,\n",
      "       0.13034030746039582, 0.1293396723117648, 0.5727171285933548,\n",
      "       0.6652388218334924, 0.4832157785469805, 0.7608074080287128,\n",
      "       0.13057836346464047, 0.8718671208885529, 0.7098554874313434,\n",
      "       0.09113698970368134, 0.139181745384776, 0.6606916127819125,\n",
      "       0.06828334846322459, 0.6062543741132707, 0.04922543828545034,\n",
      "       0.12925039238218006, 0.9026682575449486, 0.751677954446669,\n",
      "       0.3196368215453592, 0.5059958971692413, 0.8234114769709532,\n",
      "       0.12761154441696132, 0.8165169470844363, -0.0370209060341824,\n",
      "       0.16308546398161017, 0.9579813395173077, 0.39674210271618915,\n",
      "       0.06161384085263055, 0.5427142330778667, 0.06621122745375563,\n",
      "       0.7797512682760007, 0.14029340055099904, 0.4405927416366096,\n",
      "       0.03505343877773459, 0.2727098142647145, 0.4263603392821511,\n",
      "       0.35524114347562097, 0.11022688013554838, 0.08660783580690479,\n",
      "       0.10736672007695292, 0.09108192184311348, 0.09113698970368134,\n",
      "       0.3826610242275087, 0.5724710680342713, 0.12422140995731756,\n",
      "       0.08619728720840347, 0.6607050047713502, 0.5101384859019717,\n",
      "       0.8452415813184294, 0.4564777604323067, 0.03226992043652166,\n",
      "       0.09113698970368134, 0.9376045379314167, 0.11296709405115757,\n",
      "       0.08567946361681222, 0.1347272743577831, 0.3833208069103641,\n",
      "       0.006149703931846662, -0.07833201476393081, 0.09113698970368134,\n",
      "       0.31051666519762755, 0.5493454209974209, 0.7235443378437993,\n",
      "       0.23372144826164487, 0.5817507975937923, 0.09108192184311348,\n",
      "       0.5257384235303173, 0.06406513100315625, -0.02524272397010008,\n",
      "       0.09108192184311348, 0.619865700175108, 0.09103878178113811,\n",
      "       0.03650666097442279, 0.6329397066990857, 0.40819537687131985,\n",
      "       0.6636573058852213, 0.12388214622489568, 0.5924912921857803,\n",
      "       0.6836236243224776, 0.12929503234697237, -0.06192212166070166,\n",
      "       0.25922348010429597, 0.6096559550391278, 0.5307943783628575,\n",
      "       0.28802380454845355, 0.09113698970368134, 0.28285794196288916,\n",
      "       0.7615427262678061, 0.3456400626636382, 0.18548499825655163,\n",
      "       0.1700227370312336, 0.11264272221099036, 0.5594201172011195,\n",
      "       -0.0020248574703071176, 0.10329073303029512, 0.13444007868029495,\n",
      "       0.4468076225235498, 0.751677954446669, 0.31180529616864516,\n",
      "       0.3629473854836879, 0.975724449439545, 0.4295547999665995,\n",
      "       0.15704395432149598, 0.5829285748296517, 0.5571054761909267,\n",
      "       0.6144438860600729, 0.5728128341412957, 0.21878335209471633,\n",
      "       0.3494722991992203, 0.2860400800597317, 0.09650373595814854,\n",
      "       0.5609161058087917, 0.18691970955387271, 0.21902735269187112,\n",
      "       0.16973998604383855, 1.0069076832178885, -0.058944977685218,\n",
      "       -0.04154525715307955, 0.09087361391140647, 0.3958279149305226,\n",
      "       0.726175961925045, 0.08022193752994311, 0.09135572553116389,\n",
      "       -0.22253609628133286, -0.026691910430687638, 0.7215933598417199,\n",
      "       0.10195383395487623, 0.15138851248672358, 0.08197059480666458,\n",
      "       0.13251846088628738, 0.9702453109504359, 0.3289748930146621,\n",
      "       0.5025764758661468, 0.10843794015438102, 0.32518329686588454,\n",
      "       0.14081882276415758, 0.6632682110867038, 0.12929503234697237,\n",
      "       0.390965933807591, 0.07865036059084607, -0.03685246816576926,\n",
      "       0.9136716905421028, 0.28451766573429993, 0.04460196727610111,\n",
      "       0.268132779469346, 0.3356612549521596, 0.0019629959724082324,\n",
      "       0.35147039952605347, 0.6510106466488198, 0.5111741330851545,\n",
      "       0.6298506211354549, 0.4100217322133346, 0.04030813586525572,\n",
      "       0.047421713148160904, 0.7642714893112406, 0.3445504526898596,\n",
      "       0.5972450067317472, 0.3695214604774942, 0.9460626914386406,\n",
      "       0.9120831487611276, 0.1700227370312336, -0.01852518018534144,\n",
      "       0.6606916127819125, 0.8079316980061931, 0.09165481329527259,\n",
      "       -0.22253609628133286, 0.05783679771622463, 0.03483210103737011,\n",
      "       0.14571225057237192, 0.6911797985563567, 0.03848374969566437,\n",
      "       0.1453830564746097, 0.7261819258243413, 0.4783949870158147,\n",
      "       0.11260997433281872, 0.750755868797344, 0.1235964504502246,\n",
      "       0.28451766573429993, 0.13641406756764995, 1.0139549529110592,\n",
      "       0.5872187515764369, 0.19041835928503115, 1.0288986306085333,\n",
      "       0.28362486643845275, 0.15662730274611003, 0.30089024399490705,\n",
      "       -0.03438611029430716, 0.09108192184311348, 0.4372749914864335,\n",
      "       0.12434640185873613, 0.34365765339401244, 0.13178273958052822,\n",
      "       0.35000797877672857, 0.4538164075849096, 0.9419862393355587,\n",
      "       0.085581255694269, 0.12642796907426646, 0.5144619760705633,\n",
      "       0.3163700229596671, 0.5816273055951908, 0.17914618739295418,\n",
      "       0.8332173587099045, 0.34365765339401244, 0.26788617567789696,\n",
      "       0.5899807037124891, 0.6298506211354549, 0.28908239252532175,\n",
      "       0.12355181048543229, 0.11942375538384054, 0.44991404874468943,\n",
      "       0.5980802357924242, 0.7417007846295183, 0.39597658754723863,\n",
      "       0.12457092652426349, 0.09085129392901026, 0.5102179247473082,\n",
      "       0.31724378873656045, 0.049488081798987116, 0.4484349019451753,\n",
      "       0.5516479501040576, 1.0517673462328543, 1.0039628263814626,\n",
      "       1.1682436415330968, 0.6372952796089107, 0.1700227370312336,\n",
      "       0.03470815249362269, 0.3237901407083681, 0.42782783412578373,\n",
      "       0.6606916127819125, 0.2508797099172383, 0.00010770350362865333,\n",
      "       0.07380269057816158, 0.8416824286113018, 0.994221666204175,\n",
      "       0.5043888584367164, 0.1046347542316608, 0.6840917362215299,\n",
      "       0.46092001316548986, 0.6606916127819125, 0.787205386553065,\n",
      "       0.48892078606208444, 0.2907901615696331, 0.12444624450355035,\n",
      "       0.4809680774693353, -0.0319057281829217, 0.09106706566283063,\n",
      "       0.15714512633770128, 0.1402547244855029, 0.5026032598450223,\n",
      "       0.10356453671834553, 0.08073976112153436, 0.12382707836432782,\n",
      "       0.21902735269187112, 0.6934367693031269, 1.0230609648836468,\n",
      "       1.0715187081091828, 0.29122431086982004, 0.6039216655765075,\n",
      "       0.11291202619058971, 0.5427142330778667, 0.15489917485130655], dtype=object), array([1.1377479117577043, 0.4417321201256511, 0.9855134656666749,\n",
      "       0.6691537065342736, 0.08254227921946344, 0.15142623745818495,\n",
      "       0.8364201385798099, 0.097045259805558, 0.6471148104766049,\n",
      "       1.0384517305021101, 1.0606421243171111, 0.24647841644817348,\n",
      "       0.9836490235960829, 1.0441160876100117, 1.1019573386135983,\n",
      "       0.7259638674801181, 0.09692709027963231, 0.11388411008074106,\n",
      "       0.6082498705906512, 0.7490572473416914, 0.09042400079967783,\n",
      "       1.0031427295487512, 0.9158836764203764, 0.1367988612176888,\n",
      "       0.10365486909339272, 0.8229645809522891, 0.7551740007492774,\n",
      "       -0.2774628539909656, 1.0035963986085, -0.1263604310987303,\n",
      "       0.7086567825092855, 0.5243879944449956, 1.0690047610596167,\n",
      "       0.580441381619424, 0.3224633122087661, 0.45904751060765286,\n",
      "       0.08481309732971454, 0.9683838329171366, 0.09692709027963231,\n",
      "       0.4123738990878802, 0.9690890121678981, -0.01732698002836819,\n",
      "       0.3311915769819215, 0.38953145661040633, 0.9745547124249978,\n",
      "       0.2645799106194079, 0.28476324706068906, 0.21075768010932494,\n",
      "       0.7893901282744099, 0.6817456672542441, 0.5508181015495055,\n",
      "       0.2113223793918937, 0.003325739257462046, 0.13158460259655602,\n",
      "       0.445180647146828, 0.1611638815595814, 0.07440511246549286,\n",
      "       0.13363265291675896, 0.09815645184571498, 0.9891353869560842,\n",
      "       0.6952012248817648, 0.6692527175767484, 0.6692527175767484,\n",
      "       -0.05732283136292604, 0.2560575906709104, 0.5130617135912945,\n",
      "       0.0491844687711962, 0.12689844167688824, 0.08297663070760586,\n",
      "       0.7455603166003929, 0.6315349739419394, 0.6691537065342736,\n",
      "       1.0334959271616615, 0.4679535931723612, 0.11283671101675141,\n",
      "       0.15759526912940136, 0.5998861996011844, 0.6125967032117486,\n",
      "       0.9661529199314269, 0.6346979636176493, 0.6051112977215038,\n",
      "       0.18499301830339065, 0.15738452581144058, 1.033649950275244,\n",
      "       0.8004328210604236, 0.0700383521332617, 0.8587177742699047,\n",
      "       0.09692709027963231, 0.37822122870838953, 0.03771546495566236,\n",
      "       0.7086567825092855, 0.17123866427608192, 0.8729378636977767,\n",
      "       0.3869263235747512, 0.143944908042151, -0.003641117266155791,\n",
      "       1.023628186038471, 0.6092086682664948, 0.1372171292283907,\n",
      "       0.5746109765218493, 0.15344230336623232, 0.2963029563431115,\n",
      "       0.7622107942510548, 0.02294390043693917, 0.11050081743307871,\n",
      "       0.5931037738996314, 0.052727414315035026, 0.6492359755601506,\n",
      "       0.18004866011393073, -0.05792355471234223, 0.37724771759586095,\n",
      "       0.14392896818391976, 0.4477677653386067, 0.09692709027963231,\n",
      "       0.17057125934658346, 0.975733472873086, 0.25461749922489696,\n",
      "       -0.01069499363186277, 0.5949443622055186, 0.6771228426672906,\n",
      "       0.8104811566888669, 0.2511243529918872, 0.7091067997275327,\n",
      "       0.13414671334471573, 0.21833625796369038, 0.09018337160764078,\n",
      "       0.539877502440786, 0.11371053558078259, 0.09643218833512657,\n",
      "       0.72214613208665, 0.8329914337377228, 0.17125460413431315,\n",
      "       0.07013414455051659, 0.43870507974039585, 0.5508181015495055,\n",
      "       0.6279572286940048, 0.17034196286923575, 0.26289071308189205,\n",
      "       1.0328365598774398, 0.5423464678531615, 0.6642925281115191,\n",
      "       0.28885939766097624, 0.2424807258330285, 0.598327654853181,\n",
      "       0.15197868235579792, 0.06672256060164883, 0.7624790130193685,\n",
      "       0.09709315601418544, 0.6232810528126187, 0.8587390786889768,\n",
      "       0.39833840685783367, 0.6852638523011249, 0.2802654285954024,\n",
      "       0.15249024855006998, 0.05588220346794148, 0.4633887523836058,\n",
      "       0.3322837996088605, 0.097045259805558, 0.12741893453095543,\n",
      "       0.18977726363126235, 0.9057068543072468, 0.6125520307715422,\n",
      "       0.17125460413431315, 0.30414950346518854, 0.056678586922995766,\n",
      "       0.32003503736851857, 0.13002433434420058, 0.097045259805558,\n",
      "       0.02900113232991508, 0.25461749922489696, 0.2503272729551057,\n",
      "       0.17123544565086213, 0.7138569122596063, 0.09643218833512657,\n",
      "       0.030236854512503464, 0.6705726873913117, 0.8339442410071796,\n",
      "       0.636680866654826, 0.45820841589349615, 0.18004866011393073,\n",
      "       0.03925263070130913, 0.1370063859104298, 0.763476152074177,\n",
      "       -0.016106765554420588, 0.25461749922489696, -0.050965874097170016,\n",
      "       0.3606503504561412, 0.4952640113317369, 0.4477677653386067,\n",
      "       0.8878386691153983, 0.2765053073074922, 0.0835897021780373,\n",
      "       0.17095570650933145, 0.05588220346794148, 0.1435266400314491,\n",
      "       0.26008209215412126, 0.20422092024700533, 0.14413971150188054,\n",
      "       0.13917581525059874, 0.7882388053640182, 0.10244795213873514,\n",
      "       0.9830089990412159, 0.12376157160086987, 0.17152020960147119,\n",
      "       0.7162481582305962, 0.6690611327422386, 0.5355725996139107,\n",
      "       1.063279571220937, 0.5560152431033902, 0.7195268858769723,\n",
      "       0.43870507974039585, 0.10813802172367692, 0.14762673973586327,\n",
      "       0.16452682532055285, 0.097045259805558, 0.3846816851540776,\n",
      "       0.7737805109750321, 0.12353166979945807, 0.3166024502055729,\n",
      "       0.7201964917917081, 0.18382256927815777, 0.6683239015765671,\n",
      "       0.07001597502459089, 0.9744550431945671, 0.13729376316219455,\n",
      "       0.13363265291675896, 0.8806269542743264, 0.13363587154197876,\n",
      "       0.08715736896230852, 0.6125520307715422, 0.5883168956343389,\n",
      "       0.02294390043693917, 0.18684088879263927, 0.8874305590103668,\n",
      "       0.13363587154197876, 0.1477083239320608, 0.6238533539529645,\n",
      "       0.5819581874225469, 0.8946407198692516, 0.32433283990646966,\n",
      "       1.0215796022038086, 0.10198814853591154, 1.012502321427899,\n",
      "       0.8975700907380153, 0.5201135770159053, 0.5066580193883845,\n",
      "       0.1973359144316481, 0.33882963120141135, 0.19608355580910108,\n",
      "       0.7826961414104933, 0.30246050136355035, 0.01303333441751442,\n",
      "       0.3574029316117666, 0.5952825505854082, 0.2812701008276793,\n",
      "       0.1713152982098859, 0.1739993257993212, 0.63510029177012,\n",
      "       0.20996060007254347, 0.798973664964416, 0.6299397512679286,\n",
      "       0.8433581194775974, 0.4979921121788736, 0.17125460413431315,\n",
      "       0.016193744514507946, 0.2649630802884275, 0.097045259805558,\n",
      "       0.5949443622055186, 0.03570385374857332, 0.15747709960347567,\n",
      "       0.5596468643335095, 0.13363587154197876, 0.06998409530812844,\n",
      "       0.03391958260645711, 0.6869233537677378, 0.38475831908788155,\n",
      "       0.6691537065342736, 0.17777860557068237, 0.1625381578597549,\n",
      "       0.7221123401368063, 0.8347953824332165, 0.5867796253233298,\n",
      "       0.0700383521332617, 0.7357570036460357, 0.9045130489894639,\n",
      "       0.09962007285614949, 0.43250552880927995, 0.13477258268209247,\n",
      "       1.0252989446597418, 0.13828479183403963, 0.24105043235163492,\n",
      "       0.13741193268812024, 0.097045259805558, 0.04924194422154915,\n",
      "       0.801694362991539, -0.031395609126320445, 0.6498780620661684], dtype=object), array([0.17288921873572904, 0.017029471499356075, 0.7826169347451171,\n",
      "       -0.00834788848277146, 0.14702226646327576, 0.3108885947892054,\n",
      "       0.7282613397249894, 0.10147991373934684, 0.42456562246999435,\n",
      "       0.01573165869946358, 0.4377080687979221, 0.014420426447761425,\n",
      "       0.09076784824249085, 0.43391387100262946, 0.8265372507367863,\n",
      "       0.8452623383163367, 0.5427761707793358, 0.10176366299231177,\n",
      "       0.6701484791497323, 0.19216345182293948, 0.06393595343531533,\n",
      "       0.762650655281488, 0.031012470096717926, 0.5900246314025598,\n",
      "       0.8313562308775417, 0.27864891575156747, 0.10830965304269236,\n",
      "       0.3045312378928397, 0.15086412715866215, 0.13898609895957148,\n",
      "       0.13621979535340223, 0.2511979149930629, 0.20262588701522743,\n",
      "       0.9723571335628486, 0.11219197923620283, 0.19216905434425013,\n",
      "       0.1502118754949472, -0.02142649923886719, 0.45245101978710367,\n",
      "       0.43878998795321955, 0.6048200882379849, 0.7893265411051289,\n",
      "       0.08004598672034935, 0.21043572093319662, 0.5708852689614213,\n",
      "       0.05708417426028489, 0.14434213170799937, 1.0045110370815233,\n",
      "       0.6423123174769807, 0.08517557028451794, 0.7333730072709171,\n",
      "       0.3096021172412216, 0.1496842084751986, 0.3222288322348293,\n",
      "       0.10159592283331143, 0.6506044782208702, 0.10147991373934684,\n",
      "       0.8450262411958025, 0.13879182230741738, 0.7143652734271727,\n",
      "       0.7682876512706387, 0.1849389375061431, 0.10147991373934684,\n",
      "       0.6542185239424202, 0.2938783132018499, 0.2964131365782767,\n",
      "       0.192833539278782, 0.08274987348214013, 0.32844126309624794,\n",
      "       0.058765843879123314, 0.10267498784736351, 0.14209067551871934,\n",
      "       0.28316624770499393, 0.10152043976176128, 0.021087691447891466,\n",
      "       0.9019300112084861, 0.6801824437882561, 0.36363352131778337,\n",
      "       0.04298347482073672, 0.2510300512144261, 0.27145939359371773,\n",
      "       0.15508076724604103, 0.12017429729380902, 0.6766158217182481,\n",
      "       0.5216043356032645, 0.27487685134181983, 0.7142618448451331,\n",
      "       0.46372219681233046, 0.14388225528808418, -0.03384937689982803,\n",
      "       0.050833397174277706, 0.2882407608460835, 0.00471949096147295,\n",
      "       0.14892099075174725, 0.1550737892554266, 0.9652414094091514,\n",
      "       0.3619561197277804, 0.8012124263724014, 0.08517557028451794,\n",
      "       0.16309036470855964, 0.2584899375606087, 0.1383856233384385,\n",
      "       0.01573165869946358, 0.7143974459896689, 0.29828223195016934,\n",
      "       0.0265779163235198, 0.9419224684023257, 0.39247881985478317,\n",
      "       0.7258799070644841, 0.20823433455486273, 0.07056254336331014,\n",
      "       0.20382054509299463, 0.6981062442228531, 0.3549865906157873,\n",
      "       0.9423125338962163, 0.10818222986093484, 1.0111521383605995,\n",
      "       0.4298829855103038, 0.2725809645945078, 0.09559130598609289,\n",
      "       0.13855336349743874, 0.14976666953736317, 0.8764452052878877,\n",
      "       0.7955212746485584, 0.18956347935843487, 0.07474027601962141,\n",
      "       0.9059438309969322, 0.11903522235356245, 0.23496195296914224,\n",
      "       0.14926542899429462, 0.3846886239616946, 0.14407096296695954,\n",
      "       0.6510004575019039, 0.7143960369723333, 0.23716161171686478,\n",
      "       0.5981232157506426, 0.88476277514476, 0.2341958320096693,\n",
      "       0.27145939359371773, 0.2938783132018499, 0.2938783132018499,\n",
      "       0.09604954974019242, 0.48254353548488277, 0.27473870830384806,\n",
      "       0.10147991373934684, 0.10147991373934684, 0.42872557841320175,\n",
      "       0.32784571103732757, 0.8835078407933992, 0.07850830525399677,\n",
      "       0.08540201946877735, 0.1538682938734809, 0.12545849970811795,\n",
      "       0.778614475608273, 0.4275368859491184, 0.1760953535776556,\n",
      "       0.8783673078196726, 0.2232705785189737, 0.07416157247107036,\n",
      "       0.1282600774814867, 0.6341058693192166, 0.37682608792952393,\n",
      "       0.10151346177114684, 0.32116169742099065, 0.06929198618374333,\n",
      "       0.9052191683494473, 0.09926433462320727, 0.032110076238898055,\n",
      "       0.18986911943336382, 0.8472574391698215, 0.16579283274289702,\n",
      "       0.770032759255888, 0.47082227988701014, 0.7010017615972884,\n",
      "       0.14501818275360823, 0.07989921408122402, 0.12236586720692211,\n",
      "       -0.005626785247983879, 0.6348402921957192, 0.14702226646327576,\n",
      "       0.6215540219710736, 0.15508915425399106, 0.19216345182293948,\n",
      "       0.7453608271264208, 0.19216764532691444, 0.8152724924531681,\n",
      "       0.7495897403177786, 0.9591689698511909, 0.42336954566477375,\n",
      "       0.06560674549578005, 0.11783176123759564, 0.11776466517399553,\n",
      "       0.6774028249962457, 0.13103382336415026, 0.21118413561758687,\n",
      "       0.3611286698224831, 0.19216345182293948, 0.327009298306838,\n",
      "       0.2808657515025215, 0.473809463734551, 0.11754801198463072,\n",
      "       0.20818178922700237, 0.83984295562015, 0.6073760163890564,\n",
      "       0.13630879194908407, 0.5713940596353428, 0.23496195296914224,\n",
      "       0.7326641125786154, 0.4589298662351019, 0.29980248552455757,\n",
      "       0.10714485676458974, 0.0854523415164774, 0.37987362772051186,\n",
      "       0.6773091588914599, 0.20818178922700237, 0.8747808191165456,\n",
      "       0.1121947637228422, 0.037110589321793896, 0.23044462101571572,\n",
      "       0.5781125485558953, 0.08803810080324115, 0.43878998795321955,\n",
      "       0.65047867310162, 0.2521452110239687, 0.021624459956692488,\n",
      "       0.07723566384413794, 0.76495696783559, 0.10657873372796378,\n",
      "       0.3852296604689398, 0.6330222824474051, 0.06899188394260858,\n",
      "       0.19243183607733994, 0.08517557028451794, 0.459963761117329,\n",
      "       0.19216345182293948, 0.7520748407120399, 0.6948104376692469,\n",
      "       0.37454333133827294, 0.14702085744594007, 0.1282740334627155,\n",
      "       0.15490464007909077, 0.8833721428964343, 0.13871493021853165,\n",
      "       0.10142818267431108, 0.06375143929080607, 0.474143535035216,\n",
      "       0.14431837970148498, 0.33220924323584716, 0.9852237367703782,\n",
      "       0.11247224434891934, 0.16013906121224353, 0.026611464355319803,\n",
      "       -0.24136264014289732, 0.10930499702696517, 0.26588271888609794,\n",
      "       0.9347995949199426, 0.06659622240733254, -0.14485706666592546,\n",
      "       0.7321752437264852, 1.0175670238369505, 0.6576253811421522,\n",
      "       0.6822749531770087, 0.7785070738639327, 0.3066942322964604,\n",
      "       0.7031203811510092, 0.14702085744594007, -0.05351946717403833,\n",
      "       0.2634502073215853, 0.845198988368578, 0.2808657515025215,\n",
      "       0.2885222804534219, 0.7143420826271373, 0.7980685517691096,\n",
      "       0.4057815426662413, 0.10094173621321023, 0.19278936610436959,\n",
      "       0.11219197923620283, 0.8054736415825186, 0.4103324226210767,\n",
      "       -0.0006551458479208128, 0.789310178279615, 0.7388790838382583,\n",
      "       0.1436739891066694, 0.1496842084751986, 0.10147991373934684,\n",
      "       0.8339629780272803, 0.8065275710028049, 0.07469974999720697,\n",
      "       0.6549652415098858, 0.2679368502547115, 0.11783176123759564,\n",
      "       0.6757754703703867, 0.272454182208138, 0.9991582647944723,\n",
      "       0.587835137141199, 0.4847549564366718, 0.1707393208292276], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "print predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. contninued: accuracy!\n",
    "\n",
    "How did this first stab of predictions go? The possible outcomes are 1 and 0 (survival is a binary thing), but the linear regression model output doesn't match this binary format. Thus we have to map our predictions to outcomes. We'll also compute the accuracy of these results by comparing our predictions to the `Survived` column of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "# Take a look\n",
    "# print(predictions.shape)\n",
    "# print(titanic[\"Survived\"].shape)\n",
    "\n",
    "num_accurate_predictions = 0 # counter\n",
    "\n",
    "# Check whether the predictions are correct\n",
    "for i in range(predictions.shape[0]):\n",
    "    if predictions[i] == titanic[\"Survived\"][i]:\n",
    "        num_accurate_predictions +=1\n",
    "\n",
    "accuracy = float(num_accurate_predictions) / predictions.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this linear regression model is `0.783389450056` -- definitely a lot of room for improvement! Perhaps using a different model or some feature engineering could help. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. second stab: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792368125701\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the logistic regression model is `0.792368125701` -- better, but not perfect. Let's go through making a submission to kaggle before continuing to tweak the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. preparing a submission to kaggle; running the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_test = pandas.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# Age column\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "# Sex column\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "\n",
    "# Embarked column\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "# Fare column\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic[\"Fare\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a submission file\n",
    "# commented out to prevent unintentional file overwrite/creation\n",
    "# submission.to_csv(\"dataquest_logistic_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G. improving the dataquest code\n",
    "\n",
    "Brain dump of ideas:\n",
    "* Not using every feature in the model, relevant to the curse of dimensionality -- see if using the same logistic regression with less features is helpful. Perhaps things like ticket number and fare are not as useful as sex and age. \n",
    "* Try different models\n",
    "* Combine features together: perhaps combining sex and age into one feature somehow (encoding it with one digit for sex and one digit for age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper functions: Use logistic regression, try using different features\n",
    "\n",
    "def make_titanic_test_predictions(predictors):\n",
    "    # Initialize our algorithm\n",
    "    alg = LogisticRegression(random_state=1)\n",
    "    # Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "    scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "    # Take the mean of the scores (because we have one for each fold)\n",
    "    print \"accuracy\", scores.mean()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first attempt, predictors included `['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']`.    \n",
    "Let's see what happens when we do something super bare bones with just `Sex` and `Age`. I expect that this will be less accurate because while these features do seem important, there is probably more to the relationship between people and survival than `Sex and Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex', 'Age']\n",
      "accuracy 0.786756453423\n"
     ]
    }
   ],
   "source": [
    "predictors2 = ['Sex', 'Age'] \n",
    "print predictors2\n",
    "predictions2 = make_titanic_test_predictions(predictors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was it better or worse than expected? TODO: answer this after debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 'Sex', 'Age']\n",
      "accuracy 0.793490460157\n"
     ]
    }
   ],
   "source": [
    "predictors3 = ['Pclass', 'Sex', 'Age']\n",
    "print predictors3\n",
    "predictions3 = make_titanic_test_predictions(predictors3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You might notice that this is a bit of a iterative way to play with features... more quantitative measures like correlation coefficients might automate this sort of process.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
      "accuracy 0.792368125701\n"
     ]
    }
   ],
   "source": [
    "# predictors4 = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "# print predictors4\n",
    "# predictions4 = make_titanic_test_predictions(predictors4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
